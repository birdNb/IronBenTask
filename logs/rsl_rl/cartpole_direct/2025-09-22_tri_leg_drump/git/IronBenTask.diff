--- git status ---
On branch imu_back
Your branch is ahead of 'origin/imu_back' by 1 commit.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/IronBenTask/IronBenTask/tasks/direct/ironbentask/ironbentask_env.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/IronBenTask/IronBenTask/tasks/direct/ironbentask/ironbentask_env.py b/source/IronBenTask/IronBenTask/tasks/direct/ironbentask/ironbentask_env.py
index f34b2f1..eeed8f0 100644
--- a/source/IronBenTask/IronBenTask/tasks/direct/ironbentask/ironbentask_env.py
+++ b/source/IronBenTask/IronBenTask/tasks/direct/ironbentask/ironbentask_env.py
@@ -171,14 +171,6 @@ class IronbentaskEnv(DirectRLEnv):
         # ★ X 轴静止惩罚：速度 < 0.1 m/s 时扣分
         still_penalty = torch.clamp(0.1 - forward_vel, min=0.0) * -2.5   # 可调系数
 
-        # # 2. 侧向速度惩罚（y 轴）
-        # lateral_vel = self.robot.data.root_lin_vel_w[:, 1]
-        # lat_penalty = torch.abs(lateral_vel) * 0.1
-
-        # # 3. 偏航角速度惩罚（z 轴角速度）
-        # yaw_rate = self.robot.data.root_ang_vel_w[:, 2]
-        # yaw_penalty = torch.abs(yaw_rate) * 0.3
-
         # 4. roll / pitch 角度惩罚（身体倾斜） 降低惩罚1->0.5
         base_quat = self.robot.data.root_quat_w
         roll, pitch, _ = self._quat_to_euler(base_quat)
@@ -189,6 +181,10 @@ class IronbentaskEnv(DirectRLEnv):
         rew_pos = -torch.sum(ctrl_pos ** 2, dim=-1) * 0.1
         rew_vel = -torch.sum(ctrl_vel ** 2, dim=-1) * 0.05
 
+        # 6. X 正方向速度奖励
+        forward_vel = self.robot.data.root_lin_vel_w[:, 0]
+        rew_forward_vel = torch.clamp(forward_vel, min=0.0) * 2.0  # 只奖励正方向，系数可调
+
 
         current_x = self.robot.data.root_pos_w[:, 0]
         dx = current_x - self._last_x
@@ -213,11 +209,13 @@ class IronbentaskEnv(DirectRLEnv):
             - pitch_penalty
             + rew_pos
             + rew_vel
+            + rew_forward_vel  # ★ 新增正方向速度奖励
         )
 
         # TensorBoard 日志（每 16 帧一次）
         if self.log_step % 64 == 0:
             self.writer.add_scalar("reward/total",        total_reward.mean().item(),       self.log_step)
+            self.writer.add_scalar("reward/forward_vel", rew_forward_vel.mean().item(), self.log_step)
             self.writer.add_scalar("reward/forward",      rew_forward.mean().item(),        self.log_step)
             self.writer.add_scalar("reward/cum_x", self._cum_x.mean().item(), self.log_step)
             self.writer.add_scalar("penalty/still",       still_penalty.mean().item(),      self.log_step)